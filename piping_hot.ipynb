{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "deb1fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186ad823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['wesley', 'i', 'have', 'a', 'g', 'iphone', 'a...</td>\n",
       "      <td>negative emotion</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['jessedee', 'know', 'about', 'fludapp', 'awes...</td>\n",
       "      <td>positive emotion</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['swonderlin', 'can', 'not', 'wait', 'for', 'i...</td>\n",
       "      <td>positive emotion</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['sxsw', 'i', 'hope', 'this', 'year', 'festiva...</td>\n",
       "      <td>negative emotion</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['sxtxstate', 'great', 'stuff', 'on', 'fri', '...</td>\n",
       "      <td>positive emotion</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>['ipad', 'everywhere', 'sxsw', 'link']</td>\n",
       "      <td>positive emotion</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>['wave', 'buzz', 'rt', 'mention', 'we', 'inter...</td>\n",
       "      <td>no emotion toward brand or product</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160</th>\n",
       "      <td>['google', 'zeiger', 'a', 'physician', 'never'...</td>\n",
       "      <td>no emotion toward brand or product</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>['some', 'verizon', 'iphone', 'customer', 'com...</td>\n",
       "      <td>no emotion toward brand or product</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>['њпўћпаљь_ѓкѓоѓтѓјѓбввѓ_ѓјѓџв_ывrt', 'mention...</td>\n",
       "      <td>no emotion toward brand or product</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8163 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  \\\n",
       "0     ['wesley', 'i', 'have', 'a', 'g', 'iphone', 'a...   \n",
       "1     ['jessedee', 'know', 'about', 'fludapp', 'awes...   \n",
       "2     ['swonderlin', 'can', 'not', 'wait', 'for', 'i...   \n",
       "3     ['sxsw', 'i', 'hope', 'this', 'year', 'festiva...   \n",
       "4     ['sxtxstate', 'great', 'stuff', 'on', 'fri', '...   \n",
       "...                                                 ...   \n",
       "8158             ['ipad', 'everywhere', 'sxsw', 'link']   \n",
       "8159  ['wave', 'buzz', 'rt', 'mention', 'we', 'inter...   \n",
       "8160  ['google', 'zeiger', 'a', 'physician', 'never'...   \n",
       "8161  ['some', 'verizon', 'iphone', 'customer', 'com...   \n",
       "8162  ['њпўћпаљь_ѓкѓоѓтѓјѓбввѓ_ѓјѓџв_ывrt', 'mention...   \n",
       "\n",
       "                                  target company  \n",
       "0                       negative emotion   apple  \n",
       "1                       positive emotion   apple  \n",
       "2                       positive emotion   apple  \n",
       "3                       negative emotion   apple  \n",
       "4                       positive emotion  google  \n",
       "...                                  ...     ...  \n",
       "8158                    positive emotion   apple  \n",
       "8159  no emotion toward brand or product  google  \n",
       "8160  no emotion toward brand or product  google  \n",
       "8161  no emotion toward brand or product   apple  \n",
       "8162  no emotion toward brand or product  google  \n",
       "\n",
       "[8163 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('data/cleaned.csv')\n",
    "corpus.drop(columns='Unnamed: 0', inplace=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f784bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "TEST_SIZE = .25\n",
    "RANDOM_STATE = 42\n",
    "df = pd.read_csv(\n",
    "    'data/tweet_tweet.csv', \n",
    "    names=['body', 'product', 'target'],\n",
    "    header=0\n",
    ")\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f979ddb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1a/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "X_train.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3f06fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CompanyMentionDetector(BaseEstimator, TransformerMixin):\n",
    "    # List of features in 'feature_names' and the 'power' of the exponent transformation\n",
    "    def __init__(self, feature_names=None):\n",
    "        '''\n",
    "        Scan body of tweet for keywords:\n",
    "            0. copy for safety\n",
    "            1. convert to str\n",
    "            2. lowercase it all\n",
    "            3. RegexTokenize\n",
    "            4. create individual keyword columns\n",
    "            5. combine columns into 'keyword' column\n",
    "            6. drop indiviual keywrd columns\n",
    "\n",
    "        '''\n",
    "        if type(feature_names) != list:\n",
    "            feature_names = [feature_names]\n",
    "            \n",
    "        self.feature_names = feature_names\n",
    "        self.apple_words = ['apple', 'ipad', 'iphone', 'mac', 'ios']\n",
    "        self.google_words = ['google', 'android', 'pixel']\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "#         return 'something'\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for feat in self.feature_names:\n",
    "            X_copy[feat + '_string'] = X_copy[feat].astype('str')\n",
    "            X_copy[feat + '_string'] = X_copy[feat + '_string'].str.lower()\n",
    "            \n",
    "            X_copy[feat + '_token'] = X_copy[feat + '_string']. \\\n",
    "            apply(RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\").tokenize)\n",
    "            \n",
    "            X_copy[feat + '_aapl'] = X_copy[feat + '_token'].apply(self.is_apple)\n",
    "            X_copy[feat + '_goog'] = X_copy[feat + '_token'].apply(self.is_google)\n",
    "            X_copy['keyword'] = X_copy[feat + '_aapl']\n",
    "            X_copy['keyword'] = X_copy['keyword']. \\\n",
    "            combine_first(X_copy[feat + '_goog'])\n",
    "            \n",
    "            X_copy.drop(inplace=True, \n",
    "                        columns=[\n",
    "                            feat + '_string',\n",
    "                            feat + '_token', \n",
    "                            feat + '_aapl',\n",
    "                            feat + '_goog'\n",
    "            ])\n",
    "            \n",
    "            if 'product' in X_copy: X_copy.drop(inplace=True, columns='product')\n",
    "                \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "    def is_apple(self, tweet_text):\n",
    "        for keyword in self.apple_words:\n",
    "                if keyword.lower() in tweet_text:\n",
    "                    return 'apple'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "    def is_google(self, tweet_text):\n",
    "        for keyword in self.google_words:\n",
    "                if keyword.lower() in tweet_text:\n",
    "                    return 'google'\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a111f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    # List of features in 'feature_names' and the 'power' of the exponent transformation\n",
    "    def __init__(self, feature_names=None, sw=stopwords.words('english')):\n",
    "        '''\n",
    "        Another custom function that:\n",
    "            0. makes a copy for safety reasons\n",
    "            1. makes str\n",
    "            2. lowercase\n",
    "            3. tokenize\n",
    "            4. POS\n",
    "            5. POS conversion\n",
    "            6. Lemmatizes\n",
    "            7. drops irrelevant columns\n",
    "            7. returns \"feat\" + '_lemmed' column\n",
    "        '''\n",
    "        if type(feature_names) != list:\n",
    "            feature_names = [feature_names]\n",
    "        self.feature_names = feature_names\n",
    "        self.sw = sw\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        toker = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "        lemming = nltk.stem.WordNetLemmatizer()\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for feat in self.feature_names:\n",
    "            X_copy[feat + '_string'] = X_copy[feat].astype('str')\n",
    "            X_copy[feat + '_string'] = X_copy[feat + '_string'].str.lower()\n",
    "\n",
    "            X_copy[feat + '_token'] = X_copy[feat + '_string'] \\\n",
    "            .apply(toker.tokenize)\n",
    "\n",
    "            X_copy[feat + '_no_safewords'] = X_copy[feat + '_token'] \\\n",
    "            .apply(lambda row: [word for word in row if word not in self.sw])\n",
    "\n",
    "            X_copy[feat + '_tagged'] = X_copy[feat + '_no_safewords'] \\\n",
    "            .apply((lambda word: pos_tag(word)))\n",
    "\n",
    "            X_copy[feat + '_tagged'] = X_copy[feat + '_tagged'] \\\n",
    "            .apply(lambda row: [(word[0], self.nltk_to_wordnet(word[1])) for word in row])\n",
    "\n",
    "            X_copy[feat + '_lemmed'] = X_copy[feat + '_tagged'] \\\n",
    "            .apply(lambda row: [lemming.lemmatize(word[0], word[1]) for word in row])\n",
    "\n",
    "            X_copy.drop(inplace=True, \n",
    "                        columns=[\n",
    "                            feat,\n",
    "                            feat + '_string',\n",
    "                            feat + '_token', \n",
    "                            feat + '_no_safewords',\n",
    "                            feat + '_tagged'\n",
    "            ])\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "    \n",
    "    def nltk_to_wordnet(self, treebank_tag):\n",
    "        '''\n",
    "        Translate nltk POS to wordnet tags\n",
    "        '''\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac0ad829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Attending panel &amp;quot;better living through cl...</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #S...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>Google hotpot brings Netflix-style functionali...</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>4 Most Valuable Apple iPad Apps; Top Critical ...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>Domo iPhone &amp;amp; Android App: Share with Face...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>Sitting at ihop drooling over the @mention iPh...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9045</th>\n",
       "      <td>@mention you are my favorite-- thanks for comi...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>Scepticism expressed about iPad newspapers at ...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>RT @mention Watching a guy simultaneously use ...</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221</th>\n",
       "      <td>Show me your mustache and monocle.  #google #f...</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body keyword\n",
       "990   Attending panel &quot;better living through cl...  google\n",
       "25    RT @LaurieShook: I'm looking forward to the #S...   apple\n",
       "8583  Google hotpot brings Netflix-style functionali...  google\n",
       "4236  4 Most Valuable Apple iPad Apps; Top Critical ...   apple\n",
       "4524  Domo iPhone &amp; Android App: Share with Face...   apple\n",
       "...                                                 ...     ...\n",
       "8244  Sitting at ihop drooling over the @mention iPh...   apple\n",
       "9045  @mention you are my favorite-- thanks for comi...   apple\n",
       "4255  Scepticism expressed about iPad newspapers at ...   apple\n",
       "6872  RT @mention Watching a guy simultaneously use ...   apple\n",
       "7221  Show me your mustache and monocle.  #google #f...  google\n",
       "\n",
       "[6819 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = CompanyMentionDetector('body')\n",
    "xtcmd = cmd.transform(X_train)\n",
    "xtcmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae9551cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>body_lemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>google</td>\n",
       "      <td>[attend, panel, quot, well, live, cloud, compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>apple</td>\n",
       "      <td>[rt, laurieshook, look, forward, smcdallas, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>google</td>\n",
       "      <td>[google, hotpot, bring, netflix, style, functi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>apple</td>\n",
       "      <td>[valuable, apple, ipad, apps, top, critical, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>apple</td>\n",
       "      <td>[domo, iphone, amp, android, app, share, faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>apple</td>\n",
       "      <td>[sit, ihop, drool, mention, iphone, app, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9045</th>\n",
       "      <td>apple</td>\n",
       "      <td>[mention, favorite, thanks, come, mention, get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>apple</td>\n",
       "      <td>[scepticism, express, ipad, newspaper, sxsw, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>apple</td>\n",
       "      <td>[rt, mention, watch, guy, simultaneously, use,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221</th>\n",
       "      <td>google</td>\n",
       "      <td>[show, mustache, monocle, google, freedrinks, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword                                        body_lemmed\n",
       "990   google  [attend, panel, quot, well, live, cloud, compu...\n",
       "25     apple  [rt, laurieshook, look, forward, smcdallas, pr...\n",
       "8583  google  [google, hotpot, bring, netflix, style, functi...\n",
       "4236   apple  [valuable, apple, ipad, apps, top, critical, t...\n",
       "4524   apple  [domo, iphone, amp, android, app, share, faceb...\n",
       "...      ...                                                ...\n",
       "8244   apple  [sit, ihop, drool, mention, iphone, app, every...\n",
       "9045   apple  [mention, favorite, thanks, come, mention, get...\n",
       "4255   apple  [scepticism, express, ipad, newspaper, sxsw, l...\n",
       "6872   apple  [rt, mention, watch, guy, simultaneously, use,...\n",
       "7221  google  [show, mustache, monocle, google, freedrinks, ...\n",
       "\n",
       "[6819 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = TweetPreprocessor('body')\n",
    "xtcmdtp = tp.transform(xtcmd)\n",
    "xtcmdtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22bd1697",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected 2 rows, received array of length 2274",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m X_test_sparse \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(xtestvec)\n\u001b[1;32m     18\u001b[0m X_test_sparse\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(vect\u001b[38;5;241m.\u001b[39mvocabulary_)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mX_test_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m mnb \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m     23\u001b[0m mnb\u001b[38;5;241m.\u001b[39mfit(X_sparse, y_train)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1a/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1a/lib/python3.8/site-packages/pandas/core/frame.py:5553\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5548\u001b[0m             to_remove\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m   5550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   5551\u001b[0m         \u001b[38;5;66;03m# check newest element against length of calling frame, since\u001b[39;00m\n\u001b[1;32m   5552\u001b[0m         \u001b[38;5;66;03m# ensure_index_from_sequences would not raise for append=False.\u001b[39;00m\n\u001b[0;32m-> 5553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5554\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5555\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived array of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arrays[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5556\u001b[0m         )\n\u001b[1;32m   5558\u001b[0m index \u001b[38;5;241m=\u001b[39m ensure_index_from_sequences(arrays, names)\n\u001b[1;32m   5560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index\u001b[38;5;241m.\u001b[39mis_unique:\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected 2 rows, received array of length 2274"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "corpus = []\n",
    "\n",
    "for row in xtcmdtp.body_lemmed:\n",
    "    corpus.append(' '.join(row))\n",
    "\n",
    "\n",
    "\n",
    "xtcmdtpvect = vect.fit_transform(corpus)\n",
    "\n",
    "# making the X_train into a sparse matrix\n",
    "X_sparse = pd.DataFrame.sparse.from_spmatrix(xtcmdtpvect)\n",
    "X_sparse.columns = sorted(vect.vocabulary_)\n",
    "X_sparse.set_index(y_train.index, inplace=True)\n",
    "\n",
    "# making the x_test into a sparse matrix\n",
    "xtestvec = vect.transform(X_test)\n",
    "X_test_sparse = pd.DataFrame.sparse.from_spmatrix(xtestvec)\n",
    "X_test_sparse.columns = sorted(vect.vocabulary_)\n",
    "X_test_sparse.set_index(y_test.index, inplace=True)\n",
    "\n",
    "# mnb = MultinomialNB()\n",
    "\n",
    "# mnb.fit(X_sparse, y_train)\n",
    "# y_hat = mnb.predict(X_test_sparse)\n",
    "\n",
    "# precision_score(y_test, y_hat)\n",
    "# accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6ce1140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m target_prepper \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_me\u001b[39m\u001b[38;5;124m'\u001b[39m, LabelEncoder()),\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msclaer\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m      9\u001b[0m prepper \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m, body_prepper, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     ('target', target_prepper, None)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[0;32m---> 14\u001b[0m \u001b[43mprepper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1a/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:733\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;124;03m\"\"\"Transform X separately by each transformer, concatenate results.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m        sparse matrices.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     X \u001b[38;5;241m=\u001b[39m _check_X(X)\n\u001b[1;32m    736\u001b[0m     fit_dataframe_and_transform_dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/m1a/lib/python3.8/site-packages/sklearn/utils/validation.py:1345\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1341\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     ]\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "body_prepper = Pipeline([\n",
    "    ('cmd', CompanyMentionDetector('body')),\n",
    "    ('tpp', TweetPreprocessor('body'))\n",
    "])\n",
    "target_prepper = Pipeline([\n",
    "    ('label_me', LabelEncoder()),\n",
    "    ('sclaer', StandardScaler())\n",
    "])\n",
    "prepper = ColumnTransformer([\n",
    "    ('body', body_prepper, 'body')#,\n",
    "#     ('target', target_prepper, None)\n",
    "])\n",
    "\n",
    "prepper.fit_transform(X_train)\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('prepper', prepper),\n",
    "#     ('model', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bf490f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m tweet_pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoke\u001b[39m\u001b[38;5;124m'\u001b[39m, RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([a-zA-Z]+(?:’[a-z]+)?)\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      6\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcust\u001b[39m\u001b[38;5;124m'\u001b[39m, CompanyMentionDetector(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoke\u001b[39m\u001b[38;5;124m'\u001b[39m, tweet_pipe, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m----> 9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43msentiment_pipe\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# fit the pipe with transformers and basic model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepper\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor()),\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression())\n\u001b[1;32m     16\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_pipe' is not defined"
     ]
    }
   ],
   "source": [
    " cust_pipe = Pipeline([('aapl_or_goog', CompanyMentionDetector())])\n",
    "\n",
    "\n",
    "\n",
    "# cleaning_pipe = Pipeline(steps=[\n",
    "#     ('aapl_or_goog', CompanyMentionDetector(), 'body'),\n",
    "#     ('label', LabelEncoder(), 'target'), \n",
    "#     ('toke', RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\"), 'body'),\n",
    "#     ('model', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "tweet_pipe = Pipeline([\n",
    "    ('toke', RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")),\n",
    "    \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cust', CompanyMentionDetector(), 'body'),\n",
    "    ('toke', tweet_pipe, 'body'),\n",
    "    ('sent', sentiment_pipe, 'target')\n",
    "])\n",
    "\n",
    "# fit the pipe with transformers and basic model\n",
    "pipe = Pipeline([\n",
    "    ('prepper', preprocessor()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "160323c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>product</th>\n",
       "      <th>body_string</th>\n",
       "      <th>body_token</th>\n",
       "      <th>body_aapl</th>\n",
       "      <th>body_goog</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>Domo iPhone &amp;amp; Android App: Share with Face...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>domo iphone &amp;amp; android app: share with face...</td>\n",
       "      <td>[domo, iphone, amp, android, app, share, with,...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>#playhopskoch is in the apple app store (as we...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>#playhopskoch is in the apple app store (as we...</td>\n",
       "      <td>[playhopskoch, is, in, the, apple, app, store,...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>The Google Tv to IPad App: the connected tv ex...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>the google tv to ipad app: the connected tv ex...</td>\n",
       "      <td>[the, google, tv, to, ipad, app, the, connecte...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>HootSuite blog ‰ЫТ Social Media Dashboard еИ H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hootsuite blog ‰ыт social media dashboard еи h...</td>\n",
       "      <td>[hootsuite, blog, social, media, dashboard, ho...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>#SXSW GO is available on 5 platforms - iPhone,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#sxsw go is available on 5 platforms - iphone,...</td>\n",
       "      <td>[sxsw, go, is, available, on, platforms, iphon...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>RT @mention @mention has their Google Analytic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rt @mention @mention has their google analytic...</td>\n",
       "      <td>[rt, mention, mention, has, their, google, ana...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>One Day Without Shoes (Thoms Shoes) New App fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one day without shoes (thoms shoes) new app fo...</td>\n",
       "      <td>[one, day, without, shoes, thoms, shoes, new, ...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>Every game or app ad in the #SXSW Interactive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>every game or app ad in the #sxsw interactive ...</td>\n",
       "      <td>[every, game, or, app, ad, in, the, sxsw, inte...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>HootSuite Mobile for #SXSW ~ Updates for iPhon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hootsuite mobile for #sxsw ~ updates for iphon...</td>\n",
       "      <td>[hootsuite, mobile, for, sxsw, updates, for, i...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>Apple, Google AT&amp;amp;T have a surplus of cash,...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>apple, google at&amp;amp;t have a surplus of cash,...</td>\n",
       "      <td>[apple, google, at, amp, t, have, a, surplus, ...</td>\n",
       "      <td>apple</td>\n",
       "      <td>google</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body             product  \\\n",
       "4524  Domo iPhone &amp; Android App: Share with Face...  iPad or iPhone App   \n",
       "4022  #playhopskoch is in the apple app store (as we...  iPad or iPhone App   \n",
       "2743  The Google Tv to IPad App: the connected tv ex...  iPad or iPhone App   \n",
       "377   HootSuite blog ‰ЫТ Social Media Dashboard еИ H...                 NaN   \n",
       "3854  #SXSW GO is available on 5 platforms - iPhone,...                 NaN   \n",
       "...                                                 ...                 ...   \n",
       "5127  RT @mention @mention has their Google Analytic...                 NaN   \n",
       "3932  One Day Without Shoes (Thoms Shoes) New App fo...                 NaN   \n",
       "2699  Every game or app ad in the #SXSW Interactive ...                 NaN   \n",
       "894   HootSuite Mobile for #SXSW ~ Updates for iPhon...                 NaN   \n",
       "4936  Apple, Google AT&amp;T have a surplus of cash,...               Apple   \n",
       "\n",
       "                                            body_string  \\\n",
       "4524  domo iphone &amp; android app: share with face...   \n",
       "4022  #playhopskoch is in the apple app store (as we...   \n",
       "2743  the google tv to ipad app: the connected tv ex...   \n",
       "377   hootsuite blog ‰ыт social media dashboard еи h...   \n",
       "3854  #sxsw go is available on 5 platforms - iphone,...   \n",
       "...                                                 ...   \n",
       "5127  rt @mention @mention has their google analytic...   \n",
       "3932  one day without shoes (thoms shoes) new app fo...   \n",
       "2699  every game or app ad in the #sxsw interactive ...   \n",
       "894   hootsuite mobile for #sxsw ~ updates for iphon...   \n",
       "4936  apple, google at&amp;t have a surplus of cash,...   \n",
       "\n",
       "                                             body_token body_aapl body_goog  \\\n",
       "4524  [domo, iphone, amp, android, app, share, with,...     apple    google   \n",
       "4022  [playhopskoch, is, in, the, apple, app, store,...     apple    google   \n",
       "2743  [the, google, tv, to, ipad, app, the, connecte...     apple    google   \n",
       "377   [hootsuite, blog, social, media, dashboard, ho...     apple    google   \n",
       "3854  [sxsw, go, is, available, on, platforms, iphon...     apple    google   \n",
       "...                                                 ...       ...       ...   \n",
       "5127  [rt, mention, mention, has, their, google, ana...     apple    google   \n",
       "3932  [one, day, without, shoes, thoms, shoes, new, ...     apple    google   \n",
       "2699  [every, game, or, app, ad, in, the, sxsw, inte...     apple    google   \n",
       "894   [hootsuite, mobile, for, sxsw, updates, for, i...     apple    google   \n",
       "4936  [apple, google, at, amp, t, have, a, surplus, ...     apple    google   \n",
       "\n",
       "     company  \n",
       "4524   apple  \n",
       "4022   apple  \n",
       "2743   apple  \n",
       "377    apple  \n",
       "3854   apple  \n",
       "...      ...  \n",
       "5127   apple  \n",
       "3932   apple  \n",
       "2699   apple  \n",
       "894    apple  \n",
       "4936   apple  \n",
       "\n",
       "[197 rows x 7 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = CompanyMentionDetector('body')\n",
    "xtcmd = cmd.transform(X_train)\n",
    "dubs = xtcmd.loc[(xtcmd.body_aapl == 'apple') & (xtcmd.body_goog == 'google')]\n",
    "dubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d33ce427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corey']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 'corey'\n",
    "[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd0a68ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m apple_filter \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m pos_filter \u001b[38;5;241m=\u001b[39m binary_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive emotion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m apple_pos_df \u001b[38;5;241m=\u001b[39m binary_df[apple_filter \u001b[38;5;241m&\u001b[39m pos_filter]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'binary_df' is not defined"
     ]
    }
   ],
   "source": [
    "apple_filter = binary_df['company'] == 'apple'\n",
    "pos_filter = binary_df['target'] == 'positive emotion'\n",
    "\n",
    "apple_pos_df = binary_df[apple_filter & pos_filter]\n",
    "apple_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdefa83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1a",
   "language": "python",
   "name": "m1a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
