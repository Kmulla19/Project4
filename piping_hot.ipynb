{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f784bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "TEST_SIZE = .25\n",
    "RANDOM_STATE = 42\n",
    "df = pd.read_csv(\n",
    "    'data/tweet_tweet.csv', \n",
    "    names=['body', 'product', 'target'],\n",
    "    header=0\n",
    ")\n",
    "\n",
    "#dropping product\n",
    "df.drop(columns='product', inplace=True)\n",
    "#dropping null\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6e58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "        toker=RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\"),\n",
    "        lemming=WordNetLemmatizer(),\n",
    "        vectorvictor=TfidfVectorizer(),\n",
    "        sw=stopwords.words('english')\n",
    "    ):\n",
    "        '''\n",
    "        A custom transformer that occasionally works in Pipeline:\n",
    "            0. makes an X_copy for safety reasons\n",
    "            1. makes str and lowercase\n",
    "            3. tokenizes with default RegexTokenizer\n",
    "            4. tags tokens with POS and converts POS to wordnet\n",
    "            6. Lemmatizes with default WordNetLemmatizer\n",
    "            7. creates corpus via ' '.join(X_copy)\n",
    "            8. vectorizes via default TfidfVectorizer\n",
    "            R. sparse matrix object\n",
    "        '''\n",
    "        # print('init() called.')\n",
    "        self._fitted = False\n",
    "        self.toker = toker\n",
    "        self.lemming = lemming\n",
    "        self.sw = sw\n",
    "        self.vectorvictor = vectorvictor\n",
    "        \n",
    "        \n",
    "    def _clean_me(self, words, toker, lemming):\n",
    "        '''\n",
    "        this is the cleaning function\n",
    "        it represents steps 0 - 7\n",
    "        '''\n",
    "        # print('_clean_me() called.')\n",
    "        words_copy = words.copy()\n",
    "        words_copy = words.astype('string')\n",
    "        words_copy = words_copy.apply(lambda x: x.lower())\n",
    "        words_copy = words_copy.apply(lambda x: toker.tokenize(x))\n",
    "        words_copy = words_copy.apply(lambda row: \\\n",
    "        [word for word in row if word not in self.sw])\n",
    "\n",
    "        words_copy = words_copy.apply((lambda x: pos_tag(x)))\n",
    "        words_copy = words_copy.apply(lambda row: \\\n",
    "        [(word[0], self.nltk_to_wordnet(word[1])) for word in row])\n",
    "\n",
    "        words_copy = words_copy.apply(lambda row: \\\n",
    "        [lemming.lemmatize(word[0], word[1]) for word in row])\n",
    "        \n",
    "        corpus = words_copy.apply(lambda x: ' '.join(x))\n",
    "        return corpus\n",
    "         \n",
    "        \n",
    "    def fit(self, raw_doc, y=None):\n",
    "        # print('fit() called.')\n",
    "        cleaned = self._clean_me(raw_doc, self.toker, self.lemming)\n",
    "        self.vectorvictor.fit(cleaned)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # print('transform() called.')\n",
    "        if not self._fitted:\n",
    "            raise KeyError('USER != competent /n ' \\\n",
    "                           'why you no fit?')\n",
    "        X_copy = X.copy()\n",
    "        X_copy = self._clean_me(X_copy, self.toker, self.lemming)\n",
    "        vv = self.vectorvictor.transform(X_copy)\n",
    "        return vv\n",
    "    \n",
    "#         if y != None:\n",
    "#             y_copy = y.copy()\n",
    "#             y_copy = self._clean_me(y_copy, self.toker, self.lemming)\n",
    "#             return X_copy, y_copy\n",
    "#         else:\n",
    "#             return vv\n",
    "\n",
    "    \n",
    "    def nltk_to_wordnet(self, treebank_tag):\n",
    "        '''\n",
    "        Translate nltk POS to wordnet tags\n",
    "        '''\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f212dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Dummy Classifier\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Bagging\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"MultiNomial Naive Bayes\"\n",
    "    # \"QDA\",\n",
    "    # \"Gaussian Process\",\n",
    "    # \"Gaussian Naive Bayes\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    DummyClassifier(strategy=\"most_frequent\"),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=RANDOM_STATE),\n",
    "    SVC(gamma=2, C=1, random_state=RANDOM_STATE),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, random_state=RANDOM_STATE),\n",
    "    BaggingClassifier(random_state=RANDOM_STATE),\n",
    "    GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=RANDOM_STATE),\n",
    "    AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "    MultinomialNB()\n",
    "    # QuadraticDiscriminantAnalysis(),\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    # GaussianNB(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374f90ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:15, 12.30s/it]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "reg_stats = {}\n",
    "for name, classifier in tqdm(zip(names, classifiers)):\n",
    "    for_pipe = Pipeline([\n",
    "        ('exp', ExperimentalTransformer()),\n",
    "        ('for', classifier)\n",
    "    ])\n",
    "\n",
    "    for_pipe.fit(train.body, train.target)\n",
    "    preds = for_pipe.predict(test.body)\n",
    "    \n",
    "    reg_stats[name] = [\n",
    "        (' ', ' '),\n",
    "        ('acc', accuracy_score(test.target, preds)),\n",
    "        ('pre', precision_score(test.target, preds, average='weighted')),\n",
    "        ('f1', f1_score(test.target, preds, average='weighted'))\n",
    "    ]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1bc4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dummy Classifier': [(' ', ' '),\n",
       "  ('acc', 0.5807303123625165),\n",
       "  ('pre', 0.337247695696666),\n",
       "  ('f1', 0.4266985874302932)],\n",
       " 'Nearest Neighbors': [(' ', ' '),\n",
       "  ('acc', 0.5983282006159261),\n",
       "  ('pre', 0.5822768177975369),\n",
       "  ('f1', 0.5840653106969752)],\n",
       " 'Linear SVM': [(' ', ' '),\n",
       "  ('acc', 0.5807303123625165),\n",
       "  ('pre', 0.337247695696666),\n",
       "  ('f1', 0.4266985874302932)],\n",
       " 'RBF SVM': [(' ', ' '),\n",
       "  ('acc', 0.6647602287725473),\n",
       "  ('pre', 0.6620337952180668),\n",
       "  ('f1', 0.6223984301531289)],\n",
       " 'Decision Tree': [(' ', ' '),\n",
       "  ('acc', 0.6062472503299604),\n",
       "  ('pre', 0.6188293384544441),\n",
       "  ('f1', 0.48963059798442554)],\n",
       " 'Random Forest': [(' ', ' '),\n",
       "  ('acc', 0.5807303123625165),\n",
       "  ('pre', 0.337247695696666),\n",
       "  ('f1', 0.4266985874302932)],\n",
       " 'Bagging': [(' ', ' '),\n",
       "  ('acc', 0.6396832380114387),\n",
       "  ('pre', 0.6162306572415772),\n",
       "  ('f1', 0.6105050697228794)],\n",
       " 'Gradient Boosting': [(' ', ' '),\n",
       "  ('acc', 0.6423229212494501),\n",
       "  ('pre', 0.649254492358994),\n",
       "  ('f1', 0.591081363080999)],\n",
       " 'Neural Net': [(' ', ' '),\n",
       "  ('acc', 0.6432028156621206),\n",
       "  ('pre', 0.5932955255001128),\n",
       "  ('f1', 0.5807337289178823)],\n",
       " 'AdaBoost': [(' ', ' '),\n",
       "  ('acc', 0.601847778266608),\n",
       "  ('pre', 0.5779122944623908),\n",
       "  ('f1', 0.5130797701101264)],\n",
       " 'MultiNomial Naive Bayes': [(' ', ' '),\n",
       "  ('acc', 0.6339639243290806),\n",
       "  ('pre', 0.6625457486207784),\n",
       "  ('f1', 0.5598370435825994)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd99cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp = ExperimentalTransformer()\n",
    "exp.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ff14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [('exp', ExperimentalTransformer(), 'body'),\n",
    "    ('label', LabelEncoder(), 'target')],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('dum', DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "dum_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = dum_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e68af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "mnb_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = mnb_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbec04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "rfc_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = rfc_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('bag', BaggingClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "bag_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = bag_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('gbc', BaggingClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "gbc_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = gbc_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pipe = Pipeline([\n",
    "    ('exp', ExperimentalTransformer()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "ada_pipe.fit(train.body, train.target)\n",
    "print('fitted')\n",
    "preds = ada_pipe.predict(test.body)\n",
    "\n",
    "print(accuracy_score(test.target, preds))\n",
    "print(precision_score(test.target, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b235db8",
   "metadata": {},
   "outputs": [],
   "source": [
    " X, y = self._check_X_y(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ab = X_train.astype('string')\n",
    "X_ab.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e37673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CompanyMentionDetector(BaseEstimator, TransformerMixin):\n",
    "    # List of features in 'feature_names' and the 'power' of the exponent transformation\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Scan body of tweet for keywords:\n",
    "            0. copy for safety\n",
    "            1. convert to str\n",
    "            2. lowercase it all\n",
    "            3. RegexTokenize\n",
    "            4. create individual keyword columns\n",
    "            5. combine columns into 'keyword' column\n",
    "            6. drop indiviual keywrd problems\n",
    "\n",
    "        '''\n",
    "        if type(feature_names) != list:\n",
    "            feature_names = [feature_names]\n",
    "            \n",
    "        self.feature_names = feature_names\n",
    "        self.apple_words = ['apple', 'ipad', 'iphone', 'mac', 'ios']\n",
    "        self.google_words = ['google', 'android', 'pixel']\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "#         return 'something'\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for feat in self.feature_names:\n",
    "            X_copy[feat + '_string'] = X_copy[feat].astype('str')\n",
    "            X_copy[feat + '_string'] = X_copy[feat + '_string'].str.lower()\n",
    "            \n",
    "            X_copy[feat + '_token'] = X_copy[feat + '_string']. \\\n",
    "            apply(RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\").tokenize)\n",
    "            \n",
    "            X_copy[feat + '_aapl'] = X_copy[feat + '_token'].apply(self.is_apple)\n",
    "            X_copy[feat + '_goog'] = X_copy[feat + '_token'].apply(self.is_google)\n",
    "            X_copy['keyword'] = X_copy[feat + '_aapl']\n",
    "            X_copy['keyword'] = X_copy['keyword']. \\\n",
    "            combine_first(X_copy[feat + '_goog'])\n",
    "            \n",
    "            X_copy.drop(inplace=True, \n",
    "                        columns=[\n",
    "                            feat + '_string',\n",
    "                            feat + '_token', \n",
    "                            feat + '_aapl',\n",
    "                            feat + '_goog'\n",
    "            ])\n",
    "            \n",
    "            if 'product' in X_copy: X_copy.drop(inplace=True, columns='product')\n",
    "                \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "    def is_apple(self, tweet_text):\n",
    "        for keyword in self.apple_words:\n",
    "                if keyword.lower() in tweet_text:\n",
    "                    return 'apple'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "    def is_google(self, tweet_text):\n",
    "        for keyword in self.google_words:\n",
    "                if keyword.lower() in tweet_text:\n",
    "                    return 'google'\n",
    "                else:\n",
    "                    continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1a",
   "language": "python",
   "name": "m1a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
