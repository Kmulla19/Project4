{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e797d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import yaml\n",
    "import argparse\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d1ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_me_captain(corpse):\n",
    "    '''\n",
    "    \n",
    "    :corpse: a pd.series containing objects to be tokenizered\n",
    "    :return: a big ol' list that has been\n",
    "                stringified,\n",
    "                lowercased,\n",
    "                stripped of punctuation,\n",
    "                tokenized,\n",
    "                and otherwise made sexy af\n",
    "    '''\n",
    "    corpus = []\n",
    "    corpse = corpse.apply(lambda x: str(x).lower())\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    corpse = corpse.apply(lambda x: regex_token.tokenize(x))\n",
    "    # sanity check\n",
    "    # print(corpse)\n",
    "    for row in corpse:\n",
    "        for word in row:\n",
    "            corpus.append(word)\n",
    "\n",
    "    # sanity check\n",
    "    # print(corpus)\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150a7b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wesley',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'g',\n",
       " 'iphone',\n",
       " 'after',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " 'rise',\n",
       " 'austin',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'jessedee',\n",
       " 'know',\n",
       " 'about',\n",
       " 'fludapp',\n",
       " 'awesome',\n",
       " 'ipad',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'that',\n",
       " 'you',\n",
       " 'll',\n",
       " 'likely',\n",
       " 'appreciate',\n",
       " 'for',\n",
       " 'its',\n",
       " 'design',\n",
       " 'also',\n",
       " 'they',\n",
       " 're',\n",
       " 'giving',\n",
       " 'free',\n",
       " 'ts',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'swonderlin',\n",
       " 'can',\n",
       " 'not',\n",
       " 'wait',\n",
       " 'for',\n",
       " 'ipad',\n",
       " 'also',\n",
       " 'they',\n",
       " 'should',\n",
       " 'sale',\n",
       " 'them',\n",
       " 'down',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'sxsw',\n",
       " 'i',\n",
       " 'hope',\n",
       " 'this',\n",
       " 'year',\n",
       " 's',\n",
       " 'festival',\n",
       " 'isn',\n",
       " 't',\n",
       " 'as',\n",
       " 'crashy',\n",
       " 'as',\n",
       " 'this',\n",
       " 'year',\n",
       " 's',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'sxsw',\n",
       " 'sxtxstate',\n",
       " 'great',\n",
       " 'stuff',\n",
       " 'on',\n",
       " 'fri',\n",
       " 'sxsw',\n",
       " 'marissa',\n",
       " 'mayer',\n",
       " 'google',\n",
       " 'tim',\n",
       " 'o',\n",
       " 'reilly',\n",
       " 'tech',\n",
       " 'books',\n",
       " 'conferences',\n",
       " 'amp',\n",
       " 'matt',\n",
       " 'mullenweg',\n",
       " 'wordpress',\n",
       " 'teachntech',\n",
       " 'new',\n",
       " 'ipad',\n",
       " 'apps',\n",
       " 'for',\n",
       " 'speechtherapy',\n",
       " 'and',\n",
       " 'communication',\n",
       " 'are',\n",
       " 'showcased',\n",
       " 'at',\n",
       " 'the',\n",
       " 'sxsw',\n",
       " 'conference',\n",
       " 'http',\n",
       " 'ht',\n",
       " 'ly',\n",
       " 'n',\n",
       " 'm',\n",
       " 'iear',\n",
       " 'edchat',\n",
       " 'asd',\n",
       " 'nan',\n",
       " 'sxsw',\n",
       " 'is',\n",
       " 'just',\n",
       " 'starting',\n",
       " 'ctia',\n",
       " 'is',\n",
       " 'around',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'and',\n",
       " 'googleio',\n",
       " 'is',\n",
       " 'only',\n",
       " 'a',\n",
       " 'hop',\n",
       " 'skip',\n",
       " 'and',\n",
       " 'a',\n",
       " 'jump',\n",
       " 'from',\n",
       " 'there',\n",
       " 'good',\n",
       " 'time',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'android',\n",
       " 'fan',\n",
       " 'beautifully',\n",
       " 'smart',\n",
       " 'and',\n",
       " 'simple',\n",
       " 'idea',\n",
       " 'rt',\n",
       " 'madebymany',\n",
       " 'thenextweb',\n",
       " 'wrote',\n",
       " 'about',\n",
       " 'our',\n",
       " 'hollergram',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'ieavob',\n",
       " 'counting',\n",
       " 'down',\n",
       " 'the',\n",
       " 'days',\n",
       " 'to',\n",
       " 'sxsw',\n",
       " 'plus',\n",
       " 'strong',\n",
       " 'canadian',\n",
       " 'dollar',\n",
       " 'means',\n",
       " 'stock',\n",
       " 'up',\n",
       " 'on',\n",
       " 'apple',\n",
       " 'gear',\n",
       " 'excited',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'samsungmobileus',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'so',\n",
       " 'i',\n",
       " 'can',\n",
       " 'show',\n",
       " 'them',\n",
       " 'my',\n",
       " 'sprint',\n",
       " 'galaxy',\n",
       " 's',\n",
       " 'still',\n",
       " 'running',\n",
       " 'android',\n",
       " 'fail',\n",
       " 'find',\n",
       " 'amp',\n",
       " 'start',\n",
       " 'impromptu',\n",
       " 'parties',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'with',\n",
       " 'hurricaneparty',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'gvlrin',\n",
       " 'i',\n",
       " 'can',\n",
       " 't',\n",
       " 'wait',\n",
       " 'til',\n",
       " 'the',\n",
       " 'android',\n",
       " 'app',\n",
       " 'comes',\n",
       " 'out',\n",
       " 'foursquare',\n",
       " 'ups',\n",
       " 'the',\n",
       " 'game',\n",
       " 'just',\n",
       " 'in',\n",
       " 'time',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'http',\n",
       " 'j',\n",
       " 'mp',\n",
       " 'grn',\n",
       " 'pk',\n",
       " 'still',\n",
       " 'prefer',\n",
       " 'gowalla',\n",
       " 'by',\n",
       " 'far',\n",
       " 'best',\n",
       " 'looking',\n",
       " 'android',\n",
       " 'app',\n",
       " 'to',\n",
       " 'date',\n",
       " 'gotta',\n",
       " 'love',\n",
       " 'this',\n",
       " 'sxsw',\n",
       " 'google',\n",
       " 'calendar',\n",
       " 'featuring',\n",
       " 'top',\n",
       " 'parties',\n",
       " 'show',\n",
       " 'cases',\n",
       " 'to',\n",
       " 'check',\n",
       " 'out',\n",
       " 'rt',\n",
       " 'hamsandwich',\n",
       " 'via',\n",
       " 'ischafer',\n",
       " 'gt',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'axzwxb',\n",
       " 'great',\n",
       " 'sxsw',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'from',\n",
       " 'madebymany',\n",
       " 'http',\n",
       " 'tinyurl',\n",
       " 'com',\n",
       " 'nqv',\n",
       " 'l',\n",
       " 'haha',\n",
       " 'awesomely',\n",
       " 'rad',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'by',\n",
       " 'madebymany',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'htdfim',\n",
       " 'hollergram',\n",
       " 'sxsw',\n",
       " 'holler',\n",
       " 'gram',\n",
       " 'for',\n",
       " 'ipad',\n",
       " 'on',\n",
       " 'the',\n",
       " 'itunes',\n",
       " 'app',\n",
       " 'store',\n",
       " 'http',\n",
       " 't',\n",
       " 'co',\n",
       " 'kfn',\n",
       " 'f',\n",
       " 'q',\n",
       " 'via',\n",
       " 'marc',\n",
       " 'is',\n",
       " 'ken',\n",
       " 'sxsw',\n",
       " 'i',\n",
       " 'just',\n",
       " 'noticed',\n",
       " 'dst',\n",
       " 'is',\n",
       " 'coming',\n",
       " 'this',\n",
       " 'weekend',\n",
       " 'how',\n",
       " 'many',\n",
       " 'iphone',\n",
       " 'users',\n",
       " 'will',\n",
       " 'be',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'late',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'come',\n",
       " 'sunday',\n",
       " 'morning',\n",
       " 'sxsw',\n",
       " 'iphone',\n",
       " 'just',\n",
       " 'added',\n",
       " 'my',\n",
       " 'sxsw',\n",
       " 'flights',\n",
       " 'to',\n",
       " 'planely',\n",
       " 'matching',\n",
       " 'people',\n",
       " 'on',\n",
       " 'planes',\n",
       " 'airports',\n",
       " 'also',\n",
       " 'downloaded',\n",
       " 'the',\n",
       " 'klm',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'nicely',\n",
       " 'done',\n",
       " 'must',\n",
       " 'have',\n",
       " 'sxsw',\n",
       " 'app',\n",
       " 'rt',\n",
       " 'malbonster',\n",
       " 'lovely',\n",
       " 'review',\n",
       " 'from',\n",
       " 'forbes',\n",
       " 'for',\n",
       " 'our',\n",
       " 'sxsw',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'holler',\n",
       " 'gram',\n",
       " 'http',\n",
       " 't',\n",
       " 'co',\n",
       " 'g',\n",
       " 'gzypv',\n",
       " 'need',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'an',\n",
       " 'ipad',\n",
       " 'while',\n",
       " 'i',\n",
       " 'm',\n",
       " 'in',\n",
       " 'austin',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'if',\n",
       " 'i',\n",
       " 'll',\n",
       " 'need',\n",
       " 'to',\n",
       " 'q',\n",
       " 'up',\n",
       " 'at',\n",
       " 'an',\n",
       " 'austin',\n",
       " 'apple',\n",
       " 'store',\n",
       " 'oh',\n",
       " 'my',\n",
       " 'god',\n",
       " 'the',\n",
       " 'sxsw',\n",
       " 'app',\n",
       " 'for',\n",
       " 'ipad',\n",
       " 'is',\n",
       " 'pure',\n",
       " 'unadulterated',\n",
       " 'awesome',\n",
       " 'it',\n",
       " 's',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'browse',\n",
       " 'events',\n",
       " 'on',\n",
       " 'ipad',\n",
       " 'than',\n",
       " 'on',\n",
       " 'the',\n",
       " 'website',\n",
       " 'okay',\n",
       " 'this',\n",
       " 'is',\n",
       " 'really',\n",
       " 'it',\n",
       " 'yay',\n",
       " 'new',\n",
       " 'foursquare',\n",
       " 'for',\n",
       " 'android',\n",
       " 'app',\n",
       " 'kthxbai',\n",
       " 'sxsw',\n",
       " 'photo',\n",
       " 'just',\n",
       " 'installed',\n",
       " 'the',\n",
       " 'sxsw',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'which',\n",
       " 'is',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'http',\n",
       " 'tumblr',\n",
       " 'com',\n",
       " 'x',\n",
       " 't',\n",
       " 'pi',\n",
       " 'av',\n",
       " 'really',\n",
       " 'enjoying',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'gowalla',\n",
       " 'for',\n",
       " 'android',\n",
       " 'looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'seeing',\n",
       " 'what',\n",
       " 'else',\n",
       " 'they',\n",
       " 'amp',\n",
       " 'foursquare',\n",
       " 'have',\n",
       " 'up',\n",
       " 'their',\n",
       " 'sleeves',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'rt',\n",
       " 'laurieshook',\n",
       " 'i',\n",
       " 'm',\n",
       " 'looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'the',\n",
       " 'smcdallas',\n",
       " 'pre',\n",
       " 'sxsw',\n",
       " 'party',\n",
       " 'wed',\n",
       " 'and',\n",
       " 'hoping',\n",
       " 'i',\n",
       " 'll',\n",
       " 'win',\n",
       " 'an',\n",
       " 'ipad',\n",
       " 'resulting',\n",
       " 'from',\n",
       " 'my',\n",
       " 'shameless',\n",
       " 'promotion',\n",
       " 'chevysmc',\n",
       " 'rt',\n",
       " 'haha',\n",
       " 'awesomely',\n",
       " 'rad',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'by',\n",
       " 'madebymany',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'htdfim',\n",
       " 'hollergram',\n",
       " 'sxsw',\n",
       " 'via',\n",
       " 'michaelpiliero',\n",
       " 'someone',\n",
       " 'started',\n",
       " 'an',\n",
       " 'austin',\n",
       " 'partnerhub',\n",
       " 'group',\n",
       " 'in',\n",
       " 'google',\n",
       " 'groups',\n",
       " 'pre',\n",
       " 'sxsw',\n",
       " 'great',\n",
       " 'idea',\n",
       " 'the',\n",
       " 'new',\n",
       " 'sq',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'it',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'rock',\n",
       " 'update',\n",
       " 'for',\n",
       " 'iphone',\n",
       " 'and',\n",
       " 'android',\n",
       " 'should',\n",
       " 'push',\n",
       " 'tonight',\n",
       " 'http',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'etsbzk',\n",
       " 'sxsw',\n",
       " 'keepaustinweird',\n",
       " 'they',\n",
       " 'were',\n",
       " 'right',\n",
       " 'the',\n",
       " 'gowalla',\n",
       " 'app',\n",
       " 'on',\n",
       " 'android',\n",
       " 'is',\n",
       " 'sweeeeet',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'by',\n",
       " 'the',\n",
       " 'team',\n",
       " 'there',\n",
       " 'sxsw',\n",
       " 'very',\n",
       " 'smart',\n",
       " 'from',\n",
       " 'madebymany',\n",
       " 'hollergram',\n",
       " 'ipad',\n",
       " 'app',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'http',\n",
       " 't',\n",
       " 'co',\n",
       " 'a',\n",
       " 'xvwc',\n",
       " 'may',\n",
       " 'leave',\n",
       " 'my',\n",
       " 'vuvuzela',\n",
       " 'at',\n",
       " 'home',\n",
       " 'now',\n",
       " 'you',\n",
       " 'must',\n",
       " 'have',\n",
       " 'this',\n",
       " 'app',\n",
       " 'for',\n",
       " 'your',\n",
       " 'ipad',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'sxsw',\n",
       " 'http',\n",
       " 'itunes',\n",
       " 'apple',\n",
       " 'com',\n",
       " 'us',\n",
       " 'app',\n",
       " 'holler',\n",
       " 'gram',\n",
       " 'id',\n",
       " 'mt',\n",
       " 'hollergram',\n",
       " 'attn',\n",
       " 'all',\n",
       " 'sxsw',\n",
       " 'frineds',\n",
       " 'mention',\n",
       " 'register',\n",
       " 'for',\n",
       " 'gdgtlive',\n",
       " 'and',\n",
       " 'see',\n",
       " 'cobra',\n",
       " 'iradar',\n",
       " 'for',\n",
       " 'android',\n",
       " 'link',\n",
       " 'anyone',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'want',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'their',\n",
       " 'old',\n",
       " 'ipad',\n",
       " 'anyone',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'who',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'new',\n",
       " 'ipad',\n",
       " 'want',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'their',\n",
       " 'older',\n",
       " 'ipad',\n",
       " 'to',\n",
       " 'me',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'oooh',\n",
       " 'rt',\n",
       " 'mention',\n",
       " 'google',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'major',\n",
       " 'new',\n",
       " 'social',\n",
       " 'network',\n",
       " 'called',\n",
       " 'circles',\n",
       " 'possibly',\n",
       " 'today',\n",
       " 'link',\n",
       " 'the',\n",
       " 'best',\n",
       " 'rt',\n",
       " 'mention',\n",
       " 'ha',\n",
       " 'first',\n",
       " 'in',\n",
       " 'line',\n",
       " 'for',\n",
       " 'ipad',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'quot',\n",
       " 'pop',\n",
       " 'up',\n",
       " 'quot',\n",
       " 'apple',\n",
       " 'store',\n",
       " 'was',\n",
       " 'an',\n",
       " 'event',\n",
       " 'planner',\n",
       " 'eventprofs',\n",
       " 'pcma',\n",
       " 'engage',\n",
       " 'spin',\n",
       " 'play',\n",
       " 'a',\n",
       " 'new',\n",
       " 'concept',\n",
       " 'in',\n",
       " 'music',\n",
       " 'discovery',\n",
       " 'for',\n",
       " 'your',\n",
       " 'ipad',\n",
       " 'from',\n",
       " 'mention',\n",
       " 'amp',\n",
       " 'spin',\n",
       " 'com',\n",
       " 'link',\n",
       " 'itunes',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'mention',\n",
       " 'false',\n",
       " 'alarm',\n",
       " 'google',\n",
       " 'circles',\n",
       " 'not',\n",
       " 'coming',\n",
       " 'now',\n",
       " 'and',\n",
       " 'probably',\n",
       " 'not',\n",
       " 'ever',\n",
       " 'link',\n",
       " 'google',\n",
       " 'circles',\n",
       " 'social',\n",
       " 'sxsw',\n",
       " 'vatornews',\n",
       " 'google',\n",
       " 'and',\n",
       " 'apple',\n",
       " 'force',\n",
       " 'print',\n",
       " 'media',\n",
       " 'to',\n",
       " 'evolve',\n",
       " 'link',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'great',\n",
       " 'weather',\n",
       " 'to',\n",
       " 'greet',\n",
       " 'you',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'still',\n",
       " 'need',\n",
       " 'a',\n",
       " 'sweater',\n",
       " 'at',\n",
       " 'night',\n",
       " 'apple',\n",
       " 'putting',\n",
       " 'up',\n",
       " 'quot',\n",
       " 'flash',\n",
       " 'store',\n",
       " 'quot',\n",
       " 'downtown',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'ipad',\n",
       " 'hootsuite',\n",
       " 'hootsuite',\n",
       " 'mobile',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'updates',\n",
       " 'for',\n",
       " 'iphone',\n",
       " 'blackberry',\n",
       " 'amp',\n",
       " 'android',\n",
       " 'whether',\n",
       " 'you',\n",
       " 're',\n",
       " 'getting',\n",
       " 'friend',\n",
       " 'link',\n",
       " 'hey',\n",
       " 'sxsw',\n",
       " 'how',\n",
       " 'long',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'us',\n",
       " 'to',\n",
       " 'make',\n",
       " 'an',\n",
       " 'iphone',\n",
       " 'case',\n",
       " 'answer',\n",
       " 'mention',\n",
       " 'using',\n",
       " 'zazzlesxsw',\n",
       " 'and',\n",
       " 'we',\n",
       " 'll',\n",
       " 'make',\n",
       " 'you',\n",
       " 'one',\n",
       " 'mashable',\n",
       " 'the',\n",
       " 'ipad',\n",
       " 'takes',\n",
       " 'over',\n",
       " 'sxsw',\n",
       " 'video',\n",
       " 'ipad',\n",
       " 'sxsw',\n",
       " 'gadgets',\n",
       " 'link',\n",
       " 'for',\n",
       " 'i',\n",
       " 'pad',\n",
       " 'rt',\n",
       " 'mention',\n",
       " 'new',\n",
       " 'ubersocial',\n",
       " 'for',\n",
       " 'iphone',\n",
       " 'now',\n",
       " 'in',\n",
       " 'the',\n",
       " 'app',\n",
       " 'store',\n",
       " 'includes',\n",
       " 'uberguide',\n",
       " 'to',\n",
       " 'sxsw',\n",
       " 'sponsored',\n",
       " 'by',\n",
       " 'link',\n",
       " 'ipad',\n",
       " 's',\n",
       " 'smartcover',\n",
       " 'opens',\n",
       " 'to',\n",
       " 'instant',\n",
       " 'access',\n",
       " 'i',\n",
       " 'should',\n",
       " 'have',\n",
       " 'waited',\n",
       " 'to',\n",
       " 'get',\n",
       " 'one',\n",
       " 'link',\n",
       " 'apple',\n",
       " 'sxsw',\n",
       " 'hand',\n",
       " 'held',\n",
       " 'hobo',\n",
       " 'drafthouse',\n",
       " 'launches',\n",
       " 'hobo',\n",
       " 'with',\n",
       " 'a',\n",
       " 'shotgun',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'sxsw',\n",
       " 'link',\n",
       " 'hooray',\n",
       " 'rt',\n",
       " 'mention',\n",
       " 'apple',\n",
       " 'is',\n",
       " 'opening',\n",
       " 'a',\n",
       " 'pop',\n",
       " 'up',\n",
       " 'store',\n",
       " 'in',\n",
       " 'austin',\n",
       " 'for',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'link',\n",
       " 'orly',\n",
       " 'mention',\n",
       " 'google',\n",
       " 'set',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'new',\n",
       " 'social',\n",
       " 'network',\n",
       " 'circles',\n",
       " 'today',\n",
       " 'at',\n",
       " 'sxsw',\n",
       " 'wooooo',\n",
       " 'mention',\n",
       " 'apple',\n",
       " 'store',\n",
       " 'downtown',\n",
       " 'austin',\n",
       " 'open',\n",
       " 'til',\n",
       " 'midnight',\n",
       " 'sxsw',\n",
       " 'khoi',\n",
       " 'vinh',\n",
       " 'mention',\n",
       " 'says',\n",
       " 'conde',\n",
       " 'nast',\n",
       " 's',\n",
       " 'headlong',\n",
       " 'rush',\n",
       " 'into',\n",
       " 'ipad',\n",
       " 'publishing',\n",
       " 'was',\n",
       " 'a',\n",
       " 'quot',\n",
       " 'fundamental',\n",
       " 'misunderstanding',\n",
       " 'quot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'platform',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'link',\n",
       " 'lt',\n",
       " 'help',\n",
       " 'me',\n",
       " 'forward',\n",
       " 'this',\n",
       " 'doc',\n",
       " 'to',\n",
       " 'all',\n",
       " 'anonymous',\n",
       " 'accounts',\n",
       " 'techies',\n",
       " 'amp',\n",
       " 'ppl',\n",
       " 'who',\n",
       " 'can',\n",
       " 'help',\n",
       " 'us',\n",
       " 'jam',\n",
       " 'libya',\n",
       " 'sxsw',\n",
       " 'what',\n",
       " 'link',\n",
       " 'edchat',\n",
       " 'musedchat',\n",
       " 'sxsw',\n",
       " 'sxswi',\n",
       " 'classical',\n",
       " 'newtwitter',\n",
       " 'mention',\n",
       " 'mention',\n",
       " 'on',\n",
       " 'the',\n",
       " 'location',\n",
       " 'based',\n",
       " 'fast',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'future',\n",
       " 'link',\n",
       " 'via',\n",
       " 'mention',\n",
       " 'sxsw',\n",
       " 'mention',\n",
       " 'mention',\n",
       " 'google',\n",
       " 'will',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('data/tweet_tweet.csv')\n",
    "\n",
    "tweets.rename(\n",
    "    columns={\n",
    "        'tweet_text': 'body',\n",
    "        'emotion_in_tweet_is_directed_at': 'product',\n",
    "        'is_there_an_emotion_directed_at_a_brand_or_product': 'target'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "corpus = tokenize_me_captain(tweets.body)\n",
    "# sanity check\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01fb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    config_filepath = \"data/configurator.yaml\"\n",
    "    with open(config_filepath, 'r+') as f:\n",
    "        config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    config = argparse.Namespace()\n",
    "    for key, value in config_dict.items():\n",
    "        setattr(config, key, value)\n",
    "    return config\n",
    "\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5227b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Vocabulary:\n",
    "    token2index: dict = field(default_factory=dict)\n",
    "    index2token: dict = field(default_factory=dict)\n",
    "    token_counts: list = field(default_factory=list)\n",
    "    _unk_token: int = field(init=False, default=-1)\n",
    "    \n",
    "    def add(self, token):\n",
    "        if token not in self.token2index:\n",
    "            index = len(self)\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "            self.token_counts.append(0)\n",
    "        self.token_counts[self.token2index[token]] += 1\n",
    "    \n",
    "    def get_topk_subset(self, k):\n",
    "        tokens = sorted(\n",
    "            list(self.token2index.keys()),\n",
    "            key=lambda token: self.token_counts[self[token]],\n",
    "            reverse=True\n",
    "        )\n",
    "        return type(self)(\n",
    "            token2index={token: index for index, token in enumerate(tokens[:k])},\n",
    "            index2token={index: token for index, token in enumerate(tokens[:k])},\n",
    "            token_counts=[\n",
    "                self.token_counts[self.token2index[token]] for token in tokens[:k]\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def shuffle(self):\n",
    "        new_index = [_ for _ in range(len(self))]\n",
    "        random.shuffle(new_index)\n",
    "        new_token_counts = [None] * len(self)\n",
    "        for token, index in zip(list(self.token2index.keys()), new_index):\n",
    "            new_token_counts[index] = self.token_counts[self[token]]\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "        self.token_counts = new_token_counts\n",
    "\n",
    "    def get_index(self, token):\n",
    "        return self[token]\n",
    "    \n",
    "    def get_token(self, index):\n",
    "        if not index in self.index2token:\n",
    "            raise Exception(\"Invalid index.\")\n",
    "        return self.index2token[index]\n",
    "    \n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return self._unk_token\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        if token not in self.token2index:\n",
    "            return self._unk_token\n",
    "        return self.token2index[token]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdde3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Vectorizer:\n",
    "    vocab: Vocabulary\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, vocab_size):\n",
    "        vocab = Vocabulary()\n",
    "        for token in corpus:\n",
    "            vocab.add(token)\n",
    "        vocab_subset = vocab.get_topk_subset(vocab_size)\n",
    "        vocab_subset.shuffle()\n",
    "        return cls(vocab_subset)\n",
    "\n",
    "    def vectorize(self, corpus):\n",
    "        return [self.vocab[token] for token in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a5ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CooccurrenceEntries:\n",
    "    vectorized_corpus: list\n",
    "    vectorizer: Vectorizer\n",
    "    \n",
    "    @classmethod\n",
    "    def setup(cls, corpus, vectorizer):\n",
    "        return cls(\n",
    "            vectorized_corpus=vectorizer.vectorize(corpus),\n",
    "            vectorizer=vectorizer\n",
    "        )\n",
    "    \n",
    "    def validate_index(self, index, lower, upper):\n",
    "        is_unk = index == self.vectorizer.vocab.unk_token\n",
    "        if lower < 0:\n",
    "            return not is_unk\n",
    "        return not is_unk and index >= lower and index <= upper\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        window_size,\n",
    "        num_partitions,\n",
    "        chunk_size,\n",
    "        output_directory=\".\"\n",
    "    ):\n",
    "        partition_step = len(self.vectorizer.vocab) // num_partitions\n",
    "        split_points = [0]\n",
    "        while split_points[-1] + partition_step <= len(self.vectorizer.vocab):\n",
    "            split_points.append(split_points[-1] + partition_step)\n",
    "        split_points[-1] = len(self.vectorizer.vocab)\n",
    "\n",
    "        for partition_id in tqdm(range(len(split_points) - 1)):\n",
    "            index_lower = split_points[partition_id]\n",
    "            index_upper = split_points[partition_id + 1] - 1\n",
    "            cooccurr_counts = Counter()\n",
    "            for i in tqdm(range(len(self.vectorized_corpus))):\n",
    "                if not self.validate_index(\n",
    "                    self.vectorized_corpus[i],\n",
    "                    index_lower,\n",
    "                    index_upper\n",
    "                ):\n",
    "                    continue\n",
    "                \n",
    "                context_lower = max(i - window_size, 0)\n",
    "                context_upper = min(i + window_size + 1, len(self.vectorized_corpus))\n",
    "                for j in range(context_lower, context_upper):\n",
    "                    if i == j or not self.validate_index(\n",
    "                        self.vectorized_corpus[j],\n",
    "                        -1,\n",
    "                        -1\n",
    "                    ):\n",
    "                        continue\n",
    "                    cooccurr_counts[(self.vectorized_corpus[i], self.vectorized_corpus[j])] += 1 / abs(i - j)\n",
    "\n",
    "            cooccurr_dataset = np.zeros((len(cooccurr_counts), 3))\n",
    "            for index, ((i, j), cooccurr_count) in enumerate(cooccurr_counts.items()):\n",
    "                cooccurr_dataset[index] = (i, j, cooccurr_count)\n",
    "            if partition_id == 0:\n",
    "                file = h5py.File(\n",
    "                    os.path.join(\n",
    "                        output_directory,\n",
    "                        \"cooccurrence.hdf5\"\n",
    "                    ),\n",
    "                    \"w\"\n",
    "                )\n",
    "                dataset = file.create_dataset(\n",
    "                    \"cooccurrence\",\n",
    "                    (len(cooccurr_counts), 3),\n",
    "                    maxshape=(None, 3),\n",
    "                    chunks=(chunk_size, 3)\n",
    "                )\n",
    "                prev_len = 0\n",
    "            else:\n",
    "                prev_len = dataset.len()\n",
    "                dataset.resize(dataset.len() + len(cooccurr_counts), axis=0)\n",
    "            dataset[prev_len: dataset.len()] = cooccurr_dataset\n",
    "        \n",
    "        file.close()\n",
    "        with open(os.path.join(output_directory, \"vocab.pkl\"), \"wb\") as file:\n",
    "            pickle.dump(self.vectorizer.vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c70be11",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m Vectorizer\u001b[38;5;241m.\u001b[39mfrom_corpus(\n\u001b[1;32m      2\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[1;32m      3\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m cooccurrence \u001b[38;5;241m=\u001b[39m CooccurrenceEntries\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m      6\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[1;32m      7\u001b[0m     vectorizer\u001b[38;5;241m=\u001b[39mvectorizer\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mcooccurrence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_partitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcooccurrence_dir\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [7], line 32\u001b[0m, in \u001b[0;36mCooccurrenceEntries.build\u001b[0;34m(self, window_size, num_partitions, chunk_size, output_directory)\u001b[0m\n\u001b[1;32m     29\u001b[0m     split_points\u001b[38;5;241m.\u001b[39mappend(split_points[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m partition_step)\n\u001b[1;32m     30\u001b[0m split_points[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partition_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m     index_lower \u001b[38;5;241m=\u001b[39m split_points[partition_id]\n\u001b[1;32m     34\u001b[0m     index_upper \u001b[38;5;241m=\u001b[39m split_points[partition_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "vectorizer = Vectorizer.from_corpus(\n",
    "    corpus=corpus,\n",
    "    vocab_size=config.vocab_size\n",
    ")\n",
    "cooccurrence = CooccurrenceEntries.setup(\n",
    "    corpus=corpus,\n",
    "    vectorizer=vectorizer\n",
    ")\n",
    "cooccurrence.build(\n",
    "    window_size=config.window_size,\n",
    "    num_partitions=config.num_partitions,\n",
    "    chunk_size=config.chunk_size,\n",
    "    output_directory=config.cooccurrence_dir\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HDF5DataLoader:\n",
    "    filepath: str\n",
    "    dataset_name: str\n",
    "    batch_size: int\n",
    "    device: str\n",
    "    dataset: h5py.Dataset = field(init=False)\n",
    "\n",
    "    def iter_batches(self):\n",
    "        chunks = list(self.dataset.iter_chunks())\n",
    "        random.shuffle(chunks)\n",
    "        for chunk in chunks:\n",
    "            chunked_dataset = self.dataset[chunk]\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                dataset=CooccurrenceDataset(\n",
    "                    token_ids=torch.from_numpy(chunked_dataset[:,:2]).long(),\n",
    "                    cooccurr_counts=torch.from_numpy(chunked_dataset[:,\n",
    "                        2]).float()\n",
    "                ),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            for batch in dataloader:\n",
    "                batch = [_.to(self.device) for _ in batch]\n",
    "                yield batch\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def open(self):\n",
    "        with h5py.File(self.filepath, \"r\") as file:\n",
    "            self.dataset = file[self.dataset_name]\n",
    "            yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc908ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, x_max, alpha):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_size,\n",
    "            sparse=True\n",
    "        )\n",
    "        self.weight_tilde = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_size,\n",
    "            sparse=True\n",
    "        )\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(\n",
    "                vocab_size,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "        )\n",
    "        self.bias_tilde = nn.Parameter(\n",
    "            torch.randn(\n",
    "                vocab_size,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "        )\n",
    "        self.weighting_func = lambda x: (x / x_max).float_power(alpha).clamp(0, 1)\n",
    "    \n",
    "    def forward(self, i, j, x):\n",
    "        loss = torch.mul(self.weight(i), self.weight_tilde(j)).sum(dim=1)\n",
    "        loss = (loss + self.bias[i] + self.bias_tilde[j] - x.log()).square()\n",
    "        loss = torch.mul(self.weighting_func(x), loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = HDF5DataLoader(\n",
    "    filepath=os.path.join(config.cooccurrence_dir, \"cooccurrence.hdf5\"),\n",
    "    dataset_name=\"cooccurrence\",\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device\n",
    ")\n",
    "model = GloVe(\n",
    "    vocab_size=config.vocab_size,\n",
    "    embedding_size=config.embedding_size,\n",
    "    x_max=config.x_max,\n",
    "    alpha=config.alpha\n",
    ")\n",
    "model.to(config.device)\n",
    "optimizer = torch.optim.Adagrad(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate\n",
    ")\n",
    "with dataloader.open():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader.iter_batches()):\n",
    "            loss = model(\n",
    "                batch[0][:, 0],\n",
    "                batch[0][:, 1],\n",
    "                batch[1]\n",
    "            )\n",
    "            epoch_loss += loss.detach().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch}: loss = {epoch_loss}\")\n",
    "        torch.save(model.state_dict(), config.output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88f2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86357146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9135d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84498a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1a",
   "language": "python",
   "name": "m1a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
